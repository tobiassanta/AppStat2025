{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATLAS Test Beam Data\n",
    "\n",
    "Python script for analysis of ATLAS test beam data. \n",
    "\n",
    "The program reads an ASCII (i.e. text) data file, containting a large number of events, where a charged particle (electron or pion) passed through a slice of the ATLAS detector. Each passage is recorded by different detectors (essentially three independent ones!), boiling down to eleven numbers (some more relevant than others). The exercise is to separate electron and pion events based on these numbers, and in turn use this information to measure the interaction (i.e. detector performance) of pions and electrons seperately.\n",
    "\n",
    "NOTE 1: In this exercise, don't consider (at least at first) the details, but just rely on the advice from particle physicist, that in these detectors, electrons tend to give a higher signal (i.e. values) than pions do, and that none of the particle types are very rare.\n",
    "\n",
    "NOTE 2: Though the data is from particle physics, it could in principle have been from ANY other source, and the eleven numbers could for example have been indicators of cancer, key numbers for investors, or index numbers for identifying potential costumors. Thus the advice from above could be from and old doctor, economist, or marketing consultant.\n",
    "\n",
    "NOTE 3: This is real data, and while it has been prepared somewhat, it might contain surprises and does not necessarily follow known PDFs.\n",
    "\n",
    "For more information on ATLAS test beam: http://www.nbi.dk/~petersen/Teaching/Stat2025/TestBeam/TestBeamDataAnalysis.html.\n",
    "\n",
    "***\n",
    "\n",
    "### Authors: \n",
    "- Troels C. Petersen (Niels Bohr Institute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from iminuit import Minuit, cost\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters of the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write extensive output\n",
    "verbose = True\n",
    "N_verbose = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open file, `DataSet_AtlasPid_ElectronPion_2GeV.txt`, and read in all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('DataSet_AtlasPid_ElectronPion_2GeV.txt', skiprows=1, unpack=True)\n",
    "Cher, nLT, nHT, EM0, EM1, EM2, EM3, Had0, Had1, Had2, Muon = data\n",
    "Cher.shape\n",
    "\n",
    "# It is always very \"healthy\" to inspect the data visually/manually, just to get a quick feel for it.\n",
    "if (verbose) :\n",
    "    for i in range (N_verbose) :\n",
    "        print(f\"Cher: {Cher[i]:6.1f}    nLT, nHT: {int(nLT[i]):2d}, {int(nHT[i]):2d}    EM: {EM0[i]:5.2f} {EM1[i]:5.2f} {EM2[i]:5.2f} {EM3[i]:5.2f}   Had: {Had0[i]:5.2f} {Had1[i]:5.2f} {Had2[i]:5.2f}    Muon: {Muon[i]:5.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your analysis:\n",
    "\n",
    "Through plotting and making selections in one/several variable(s) seeing the impact (e.g. how many events remain) in others, work out what this sample consists of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example plot could be to see if there are any clear types difference between the entries in the data, and if this gives rise to a way of selecting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(16, 10))\n",
    "\n",
    "ax[0,0].hist(Cher, bins=200, range=(400, 1400), histtype='step', label='Cherenkov')\n",
    "ax[0,0].set(xlabel=\"Cherenkov\", ylabel=\"Frequency\")\n",
    "\n",
    "ax[1,1].hist(EM0,  bins=200, range=(-0.5, 2.0), histtype='step', label='EM0')\n",
    "ax[1,1].hist(EM1,  bins=200, range=(-0.5, 2.0), histtype='step', label='EM1')\n",
    "ax[1,1].hist(EM2,  bins=200, range=(-0.5, 2.0), histtype='step', label='EM2')\n",
    "ax[1,1].hist(EM3,  bins=200, range=(-0.5, 2.0), histtype='step', label='EM3')\n",
    "ax[1,1].set(xlabel=\"Energy in ElectroMagnetic Calorimeter Layer 0-3 (EM0-3)\", ylabel=\"Frequency\")\n",
    "\n",
    "h = ax[0,1].hist2d(EM1,  Cher, bins=(100, 100), range=((-0.5, 2.0), (400, 1400)), norm=mpl.colors.LogNorm(), cmap=\"Reds\")\n",
    "plt.colorbar(h[3], ax=ax[0, 1])                    # z-scale on the right of the figure\n",
    "ax[0,1].set(xlabel=\"EM1\", ylabel=\"Cherenkov\")\n",
    "\n",
    "# You can add your own fourth plot:\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The simple plotting function gives a warning for EM1 (not Cher). Ask yourself, if you did a check of the original data file input values in any way? That is always \"healthy\"!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to be answered:\n",
    "\n",
    "Generally, this analysis is about separating electrons and pions (and determining how well this can be done), followed by a few questions characterizing the detector response to each type of particle. Thus, you should imagine, that your new detector/equipment/questionaire gave you this output, and now it is up to you to find out, what this tells you about your experiment, and how to extract information from it in the best possible way. Typically, it will have taken you months (if not years) to get to this point.\n",
    "\n",
    "Note that this data is NOT meant for “fine tuned analysis”, but rather “crude inspection”. You should try to get simple approximate answers out - I’m sure that you will afterwards be able to fine tune them.\n",
    "\n",
    "Below are questions guiding you, some/most of which your analysis should cover, but you do **not** have to follow them blindly (I've put \"Optional\" on those that are not essential). Start by considering the data, and get a feel for the typical range of each variable. Plot the variables, both in 1D and also 2D! From considering these plots, guess/estimate an approximate knowledge of how electrons and pions distribute themselves in the variables above, and how to make a selection of these.\n",
    "\n",
    "As described on the webpage introducing the data, there are three (relevant) detectors:\n",
    "- Cherenkov (1 value),\n",
    "- TRT (Transition Radiation Tracker, 2 values) and\n",
    "- Calorimeters (4 values)\n",
    "\n",
    "They are each capable of separating electrons and pions. As they are (largely) _INDEPENDENT_ (three separate detectors), they may be used to cross check each other, and this is what you should use, in fact the essential part of this (and many other) analysis!\n",
    "\n",
    "\n",
    "Questions:\n",
    "----------\n",
    "1. Find for each of these three detector systems one variable, which seem to separate electrons and pions best. For example, start with the Cherenkov, which is only a single number, and assume/guess that the large peak at low values is mainly from pions, while the upper broad peak is from electrons (this you would know, as you designed the experiment). Now plot the TRT and Calorimeter distributions when the Cherenkov selects a pion and afterwards an electron. This should give you a good idea about how to separate pions and electrons using the TRT and Calorimeters.\n",
    "\n",
    "    Hint: Sometimes variables from a single detector are more powerful, when they are combined, e.g. taken ratios of (or used in a Fisher or ML algorithm). For the TRT this may be somewhat doable, but for the EMcalo, it is not as simple. Here, one variable caries most of the separation power, but involving other layers may enhance the separation power. However, to begin with, just consider a single number from each detector.\n",
    "\n",
    "\n",
    "2. Next you should try to see, if you can make a selection, which gives you a fairly large and clean electron and pion sample, respectively. The question is, how can you know how clean your sample is and how efficient your selection is?  This can actually be measured in the data itself, using the fact that there are three independent detectors. For example, start by making an electron and a pion selection using two of the three variables, and plot the third variable for each of these selections. Now you can directly see, how electrons and pions will distribute themselves in this third variable. Are you worried, that there are pions in your electron sample, and vice versa? Well, there will probably be, but so few, that it won't matter too much, at least not to begin with. Why? Well, let us assume that for each detector, 80% of electrons pass your requirement, but also 10% of pions do. Assuming an even number of electrons and pions (which is not really the case), then with two detector cuts, you should get a sample, which is: $\\frac{0.8\\cdot0.8} {0.8\\cdot0.8 + 0.1\\cdot0.1} = 98.5\\%$ pure.\n",
    "\n",
    "    Now with this sample based on cuts on the two other detectors, ask what fraction of electrons and pions passes your electron selection in the remaining detector. The fraction of electrons, that are not selected as electrons will be your TYPE I errors (False Negative Rate), denoted alpha, while the fraction of pions, that do pass the cut will be your TYPE II errors (False Positive Rate), denoted beta. Measure these for each of the two cuts in the three detector types, and ask yourself if they are \"reasonable\", i.e. something like in the example above. If not, then you should perhaps reconsider adjusting your cuts.\n",
    "\n",
    "By now, you should for each detector have 6 numbers (first considering electrons and then pions as \"Positive\"):\n",
    "    - The electron cut value above which you accept an electron.\n",
    "    - The efficiency (i.e. True Positive Rate) for electrons of this cut.\n",
    "    - The fake rate (i.e. False Positive Rate) for pions of this cut.\n",
    "    - The pion cut value below which you accept a pion (may be same value as above for electrons!).\n",
    "    - The efficiency (i.e. True Positive Rate) for pions of this cut.\n",
    "    - The fake rate (i.e. False Positive Rate) for electrons of this cut.\n",
    "\n",
    "\n",
    "3. Given the efficiencies and fake rates of each cut, try to combine these (again assuming that they are independent) into knowledge of your sample purities and also the total number of electrons and pions in the whole sample. Do the sum of estimated electrons and pions added actually match the number of particles in total? This is a good cross check!\n",
    "\n",
    "### More advanced questions:\n",
    "\n",
    "4. If the number of pions was suddenly 1000 times that of elections, would you still be able to get a sample of fairly pure (say 90% pure) electrons? And if so, what would the efficiency for these electrons be? That is roughly equivalent of asking, if you can get a 99.9% pure electron sample from the data given (where pions do not dominate in numbers).\n",
    "\n",
    "\n",
    "5. Expanding on problem 2), try now to calculate ROC curves for each of the three detectors. These are obtained by making a clean selection using the two other detectors for electrons and pions seperately, and then integrating over these two distributions, using the running (normalised) integral of each as x and y coordinate in a (ROC) curve. If you do not manage on your own, perhaps consider the ROC calculator example, which is posted along with this exercise.\n",
    "\n",
    "\n",
    "6. One of the purposes of the testbeam was to measure the response of the TRT detector to exactly electrons and pions. Consider for example only events that has 33 TRT hits (i.e. `nLT` $= 33$). As the High-Threshold probability (i.e. probability of passing the High-Threshold, given that the Low-Threshold was passed), is assumed to be constant in the TRT detector (but quite different for electrons and pions), what distribution should the number of High-Threshold hits (`nHT`) follow? And is that really the case, both for electrons and pions?\n",
    "\n",
    "\n",
    "7. Still considering `nLT` $=33$, and given that there are both electrons and pions in the sample (each with a different HT probability), `nHT` should in the unselected data be a combination of two distributions. Try to fit the number of HT hits with two distributions combined. Do they fit the data? And can this fit be used to estimate the fraction of pions and electrons in the sample? And does that estimate match you previous estimate? Perhaps retry with other values for the number of TRT hits.\n",
    "\n",
    "\n",
    "8. Try to select pions using three different (mutually exclusive) techniques:\n",
    "    1. Passing only a hadronic calorimeter requirement (e.g. that the sum of the three HCal values is above some minimum energy).\n",
    "    2. Passing only Cherenkov AND EMcalo requirements.\n",
    "    3. Passing both A) and B).\n",
    "\n",
    "Try to measure the HT probability (i.e. fraction of High-Threshold hits) for each of these three pion samples. Do they agree with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "This exercise might be the closest thing to reality regarding real data that is not easily modelled. Note that the original raw data is a lot more messy, and that a lot of cleaning has taken place, before you got it!\n",
    "\n",
    "From this exercise you should realise:\n",
    "1. that real data is not as clean-cut, well-behaved, and Gaussian as you might think.\n",
    "2. that the use of **independent variables is very powerful** in data analysis.\n",
    "3. that you need to be sharp regarding TPR, FPR, TNR, and FNR... :-)\n",
    "4. that the sign of a good analysis is, that cross checks have been thought into it (e.g. end of Q.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example solution\n",
    "\n",
    "As our data is unlabeled, we first need to establish labels, so we later on can determine other detectors efficiacy of distinhushing between electrons and pions. Here we can use our knowledge about electrons and pions and their expected mesaurement in the cherikov and EM1 detector, and try to \"cut out\" the pions as seen below. This mask is not perfect, but it should label most of the electrons and pions correct. We check this later\n",
    "\n",
    "In blue the pions are marked, and in orange the electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pions = (Cher < 640) * (EM1 < 0.35)\n",
    "electrons = ~Pions # We make labels\n",
    "\n",
    "plt.scatter(EM1[Pions], Cher[Pions], alpha = 0.1)\n",
    "plt.scatter(EM1[electrons], Cher[electrons], alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing different detectors\n",
    "As we now have a labeling, we can use this to compute the TPR and FPR, of the different detectors on their own. Below is shown 6 histograms for the first detectors, with a cutoff shown to distinguish electrons and pions, given the distributions.\n",
    "Looking at the results, many of the detectros give a poor clasification, bordering to a random classifier, but two detectors seem very promissing, EM1 and nHT. Here both of the detctors measure a high TPR, while having a low FPR.\n",
    "\n",
    "On the bottom plot we have shown the distribution of the hadron measurement. Electrons are not expected to reach this, making it a bad detector to classify between the two, but we can use it as a type of sanity to check to see that our initial labeling was good, as we then only would expect pions represented in the plots. Looking at the plots a majority of the particles reached is pions, while only a few \"electrons\" made it thorugh. Computing the fraction of the labeled pions that made it through relative to the total amount of particles that made it thorugh we end up with a fraction in the high 80 and 90, suggesting that the cut we made in the beginning was great at labeling pions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, figsize = (15, 3))\n",
    "ax[0].hist(nLT[Pions], bins = np.arange(100), histtype = 'step')\n",
    "ax[0].hist(nLT[electrons], bins = np.arange(100), histtype = 'step')\n",
    "ax[0].set_title('nLT')\n",
    "ax[0].vlines(37, 0, 2000, color = 'tab:red', ls = '--')\n",
    "tp = sum(nLT[Pions] < 37)\n",
    "fp = sum(nLT[electrons] < 37)\n",
    "fn = sum(nLT[Pions] > 37)\n",
    "tn = sum(nLT[electrons] > 37)\n",
    "FPR = fp/(fp + tn)\n",
    "TPR = tp/(tp + fn)\n",
    "ax[0].text(0.1, 0.9, f\"TPR = {TPR:3.5f} \\nFPR = {FPR:3.5f}\", family='monospace', fontsize=7, transform = ax[0].transAxes)\n",
    "\n",
    "ax[1].hist(nHT[Pions], bins = np.arange(40), histtype = 'step')\n",
    "ax[1].hist(nHT[electrons], bins = np.arange(40), histtype = 'step')\n",
    "ax[1].set_title('nHT')\n",
    "ax[1].vlines(3, 0, 4000, color = 'tab:red', ls = '--')\n",
    "tp = sum(nHT[Pions] < 3)\n",
    "fp = sum(nHT[electrons] < 3)\n",
    "fn = sum(nHT[Pions] > 3)\n",
    "tn = sum(nHT[electrons] > 3)\n",
    "FPR = fp/(fp + tn)\n",
    "TPR = tp/(tp + fn)\n",
    "ax[1].text(0.1, 0.9, f\"TPR = {TPR:3.5f} \\nFPR = {FPR:3.5f}\", family='monospace', fontsize=7, transform = ax[1].transAxes)\n",
    "\n",
    "\n",
    "ax[2].hist(EM1[Pions], bins = np.linspace(-0.5, 2.0, 100), histtype = 'step')\n",
    "ax[2].hist(EM1[electrons], bins = np.linspace(-0.5, 2.0, 100), histtype = 'step')\n",
    "ax[2].set_title('EM1')\n",
    "ax[2].vlines(0.3, 0, 1200, color = 'tab:red', ls = '--')\n",
    "tp = sum(EM1[Pions] < 0.3)\n",
    "fp = sum(EM1[electrons] < 0.3)\n",
    "fn = sum(EM1[Pions] > 0.3)\n",
    "tn = sum(EM1[electrons] > 0.3)\n",
    "FPR = fp/(fp + tn)\n",
    "TPR = tp/(tp + fn)\n",
    "ax[2].text(0.1, 0.9, f\"TPR = {TPR:3.5f} \\nFPR = {FPR:3.5f}\", family='monospace', fontsize=7, transform = ax[2].transAxes)\n",
    "\n",
    "\n",
    "ax[3].hist(EM2[Pions], bins = np.linspace(-0.5, 2.0, 100), histtype = 'step')\n",
    "ax[3].hist(EM2[electrons], bins = np.linspace(-0.5, 2.0, 100), histtype = 'step')\n",
    "ax[3].set_title('EM2')\n",
    "ax[3].vlines(0.33, 0, 1200, color = 'tab:red', ls = '--')\n",
    "tp = sum(EM2[Pions] < 0.33)\n",
    "fp = sum(EM2[electrons] < 0.33)\n",
    "fn = sum(EM2[Pions] > 0.33)\n",
    "tn = sum(EM2[electrons] > 0.33)\n",
    "FPR = fp/(fp + tn)\n",
    "TPR = tp/(tp + fn)\n",
    "ax[3].text(0.1, 0.9, f\"TPR = {TPR:3.5f} \\nFPR = {FPR:3.5f}\", family='monospace', fontsize=7, transform = ax[3].transAxes)\n",
    "\n",
    "\n",
    "ax[4].hist(EM3[Pions], bins = np.linspace(-0.5, 2.0, 100), histtype = 'step')\n",
    "ax[4].hist(EM3[electrons], bins = np.linspace(-0.5, 2.0, 100), histtype = 'step')\n",
    "ax[4].set_title('EM3')\n",
    "ax[4].vlines(0.03, 0, 5000, color = 'tab:red', ls = '--')\n",
    "tp = sum(EM3[Pions] < 0.03)\n",
    "fp = sum(EM3[electrons] < 0.03)\n",
    "fn = sum(EM3[Pions] > 0.03)\n",
    "tn = sum(EM3[electrons] > 0.03)\n",
    "FPR = fp/(fp + tn)\n",
    "TPR = tp/(tp + fn)\n",
    "ax[4].text(0.1, 0.9, f\"TPR = {TPR:3.5f} \\nFPR = {FPR:3.5f}\", family='monospace', fontsize=7, transform = ax[4].transAxes)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize = (15, 3))\n",
    "\n",
    "ax[0].hist(Had0[Pions], bins = np.linspace(0.01,2, 50), histtype = 'step')\n",
    "ax[0].hist(Had0[electrons], bins = np.linspace(0.01,2, 50), histtype = 'step')\n",
    "ax[0].set_title('Had0')\n",
    "freq_pions, _ = np.histogram(Had0[Pions], bins = np.linspace(0.01, 2, 50))\n",
    "freq_electrons, _ = np.histogram(Had0[electrons], bins = np.linspace(0.01, 2, 50))\n",
    "print(freq_pions.sum()/(freq_pions.sum() + freq_electrons.sum()))\n",
    "\n",
    "\n",
    "\n",
    "ax[1].hist(Had1[Pions], bins = np.linspace(0.01,2, 50), histtype = 'step')\n",
    "ax[1].hist(Had1[electrons], bins = np.linspace(0.01,2, 50), histtype = 'step')\n",
    "ax[1].set_title('Had1')\n",
    "freq_pions, _ = np.histogram(Had1[Pions], bins = np.linspace(0.01, 2, 50))\n",
    "freq_electrons, _ = np.histogram(Had1[electrons], bins = np.linspace(0.01, 2, 50))\n",
    "print(freq_pions.sum()/(freq_pions.sum() + freq_electrons.sum()))\n",
    "\n",
    "ax[2].hist(Had2[Pions], bins = np.linspace(0.01,2, 50), histtype = 'step')\n",
    "ax[2].hist(Had2[electrons], bins = np.linspace(0.01,2, 50), histtype = 'step')\n",
    "ax[2].set_title('Had2')\n",
    "freq_pions, _ = np.histogram(Had2[Pions], bins = np.linspace(0.01, 2, 50))\n",
    "freq_electrons, _ = np.histogram(Had2[electrons], bins = np.linspace(0.01, 2, 50))\n",
    "print(freq_pions.sum()/(freq_pions.sum() + freq_electrons.sum()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining measurements\n",
    "We have seen that individual detectors can make a great selection on their own, but now we want to investigate if we can combine them to create an even beter destinction. To do this we use a LDA, where we can include and exclude different detectors as we choose. In the end we compute a roc curve, making the comparison between them better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(data, include, pmask, emask):\n",
    "    data = data[include,:]\n",
    "    pions = data[:, pmask]\n",
    "    electrons = data[:,emask]\n",
    "    pions_nanmask = np.ones(sum(pmask))\n",
    "    electrons_nanmask = np.ones(sum(emask))\n",
    "    for i in range(len(include)):\n",
    "        pions_nanmask *= ~np.isnan(pions[i,:])\n",
    "        electrons_nanmask *= ~np.isnan(electrons[i,:])\n",
    "    pions = pions[:, pions_nanmask.astype(bool)]\n",
    "    electrons = electrons[:, electrons_nanmask.astype(bool)]\n",
    "\n",
    "    mean_pions = np.mean(pions, axis = 1)\n",
    "    mean_electrons = np.mean(electrons, axis = 1)\n",
    "\n",
    "    cov_pions = np.cov(pions)\n",
    "    cov_electrons = np.cov(electrons)\n",
    "    wf = np.linalg.inv(cov_pions + cov_electrons) @ (mean_pions - mean_electrons)\n",
    "    pions_proj = pions.T @ wf\n",
    "    electrons_proj = electrons.T @ wf\n",
    "    return pions_proj, electrons_proj, wf\n",
    "\n",
    "def compute_rates(TA, TB, thr):\n",
    "    tp = sum(TB < thr)\n",
    "    fp = sum(TA < thr)\n",
    "    fn= sum(TB > thr)\n",
    "    tn = sum(TA > thr)\n",
    "\n",
    "\n",
    "    \n",
    "    FPR = fp/(fp + tn)\n",
    "    TPR = tp/(tp + fn)\n",
    "    return TPR, FPR\n",
    "\n",
    "def calc_ROC(hist1, hist2) :\n",
    "\n",
    "    # First we extract the entries (y values) and the edges of the histograms:\n",
    "    # Note how the \"_\" is simply used for the rest of what e.g. \"hist1\" returns (not really of our interest)\n",
    "    y_sig, x_sig_edges = hist1 \n",
    "    y_bkg, x_bkg_edges = hist2\n",
    "    \n",
    "    # Check that the two histograms have the same x edges:\n",
    "    if np.array_equal(x_sig_edges, x_bkg_edges) :\n",
    "        \n",
    "        # Extract the center positions (x values) of the bins (both signal or background works - equal binning)\n",
    "        x_centers = 0.5*(x_sig_edges[1:] + x_sig_edges[:-1])\n",
    "        \n",
    "        # Calculate the integral (sum) of the signal and background:\n",
    "        integral_sig = y_sig.sum()\n",
    "        integral_bkg = y_bkg.sum()\n",
    "    \n",
    "        # Initialize empty arrays for the True Positive Rate (TPR) and the False Positive Rate (FPR):\n",
    "        TPR = np.zeros_like(y_sig) # True positive rate (sensitivity)\n",
    "        FPR = np.zeros_like(y_sig) # False positive rate ()\n",
    "        \n",
    "        # Loop over all bins (x_centers) of the histograms and calculate TN, FP, FN, TP, FPR, and TPR for each bin:\n",
    "        for i, x in enumerate(x_centers): \n",
    "            \n",
    "            # The cut mask\n",
    "            cut = (x_centers < x)\n",
    "            \n",
    "            # True positive\n",
    "            TP = np.sum(y_sig[~cut]) / integral_sig    # True positives\n",
    "            FN = np.sum(y_sig[cut]) / integral_sig     # False negatives\n",
    "            TPR[i] = TP / (TP + FN)                    # True positive rate\n",
    "            # print(TP, FN, TPR[i])\n",
    "            \n",
    "            # True negative\n",
    "            TN = np.sum(y_bkg[cut]) / integral_bkg      # True negatives (background)\n",
    "            FP = np.sum(y_bkg[~cut]) / integral_bkg     # False positives\n",
    "            FPR[i] = FP / (FP + TN)                     # False positive rate            \n",
    "        return FPR, TPR\n",
    "    \n",
    "    else:\n",
    "        AssertionError(\"Signal and Background histograms have different bins and/or ranges\")\n",
    "\n",
    "\n",
    "\n",
    "def ROC(data, include, pmask, emask):\n",
    "    pionsLDA, electronsLDA, wf = LDA(data, include, Pions, electrons)\n",
    "\n",
    "    PionsHist = np.histogram(pionsLDA, np.linspace(-40,10,100))\n",
    "    ElectronsHist = np.histogram(electronsLDA, np.linspace(-40,10,100))\n",
    "    return calc_ROC(PionsHist, ElectronsHist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = ROC(data, [1,2,3], Pions, electrons)\n",
    "res2 = ROC(data, [1,2,3,4,5,6], Pions, electrons)\n",
    "res3 = ROC(data, [1,2,3,8,9,10], Pions, electrons)\n",
    "# print(res2[0])\n",
    "\n",
    "plt.plot(res1[0], res1[1])\n",
    "plt.plot(res2[0], res2[1])\n",
    "plt.plot(res3[0], res3[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "executable": "/usr/bin/env python",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
