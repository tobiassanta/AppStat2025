{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-parameters discriminant analysis\n",
    "\n",
    "Python notebook for constructing a Fisher disciminant from two 2D Gaussianly distributed correlated variables. The notebook creates artificial random data for two different types of processes, and the goal is then to separate these by constructing a Fisher discriminant.\n",
    "\n",
    "### Authors: \n",
    "- Christian Michelsen (Niels Bohr Institute)\n",
    "- Troels C. Petersen (Niels Bohr Institute)\n",
    "\n",
    "### References:\n",
    "- Glen Cowan, Statistical Data Analysis, pages 51-57\n",
    "- http://en.wikipedia.org/wiki/Linear_discriminant_analysis\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                     # Matlab like syntax for linear algebra and functions\n",
    "import matplotlib.pyplot as plt                        # Plots and figures like you know them from Matlab\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random             # Random generator\n",
    "r.seed(42)                # Set a random seed (but a fixed one)\n",
    "save_plots = False          # For now, don't save plots (once you trust your code, switch on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:\n",
    "\n",
    "Function to calculate the separation betweem two lists of numbers (see equation at the bottom of the script).\n",
    "\n",
    "__Note__: You need to fill in this function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_separation(x, y):\n",
    "    d = np.abs((np.mean(x) - np.mean(y))) / np.sqrt(np.std(x, ddof=1)**2 + np.std(y, ddof=1)**2)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters:\n",
    "\n",
    "\n",
    "Number of species, their means and widths, correlations and the number of observations of each species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of 'species': signal / background\n",
    "n_spec = 2       \n",
    "\n",
    "# Species A, mean and width for the two dimensions/parameters\n",
    "mean_A  = [15.0, 50.0] \n",
    "width_A = [ 2.0,  6.0] \n",
    "\n",
    "# Species B, mean and width for the two dimensions/parameters\n",
    "mean_B  = [12.0, 55.0] \n",
    "width_B = [ 3.0,  6.0] \n",
    "\n",
    "# Coefficient of correlation\n",
    "corr_A = 0.8\n",
    "corr_B = 0.9\n",
    "\n",
    "# Amount of data you want to create\n",
    "n_data = 2000         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data:\n",
    "\n",
    "For each \"species\", produce a number of $(x_0,x_1)$ points which are (linearly) correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# The desired covariance matrix.\n",
    "V_A = np.array([[width_A[0]**2, width_A[0]*width_A[1]*corr_A],\n",
    "                [width_A[0]*width_A[1]*corr_A, width_A[1]**2]])\n",
    "V_B = np.array([[width_B[0]**2, width_B[0]*width_B[1]*corr_B],\n",
    "                [width_B[0]*width_B[1]*corr_B, width_B[1]**2]])\n",
    "\n",
    "# Generate the random samples.\n",
    "spec_A = np.random.multivariate_normal(mean_A, V_A, size=n_data)\n",
    "spec_B = np.random.multivariate_normal(mean_B, V_B, size=n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Plot your generated data:\n",
    "\n",
    "We plot the 2D-data as 1D-histograms (basically projections) in $x_0$ and $x_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1D, ax_1D = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "\n",
    "ax_1D[0].hist(spec_A[:, 0], 50, range=(0, 25), histtype='step', label='Species A', color='Red', lw=1.5)\n",
    "ax_1D[0].hist(spec_B[:, 0], 50, range=(0, 25), histtype='step', label='Species B', color='Blue', lw=1.5)\n",
    "ax_1D[0].set(title='Parameter x0', xlabel='x0', ylabel='Counts', xlim=(0,25))\n",
    "ax_1D[0].legend(loc='upper left')\n",
    "\n",
    "# uncomment later\n",
    "#ax_1D[0].text(1, 176, fr'$\\Delta_{{x0}} = {calc_separation(spec_A[:, 0], spec_B[:, 0]):.3f}$', fontsize=16)\n",
    "\n",
    "ax_1D[1].hist(spec_A[:, 1], 50, range=(20, 80), histtype='step', label='Species A', color='Red', lw=1.5)\n",
    "ax_1D[1].hist(spec_B[:, 1], 50, range=(20, 80), histtype='step', label='Species B', color='Blue', lw=1.5)\n",
    "ax_1D[1].set(title='Parameter x1', xlabel='x1', ylabel='Counts', xlim=(20, 80))\n",
    "ax_1D[1].legend(loc='upper left')\n",
    "\n",
    "# uncomment later\n",
    "#ax_1D[1].text(22, 140, fr'$\\Delta_{{x1}} = {calc_separation(spec_A[:, 1], spec_B[:, 1]):.3f}$', fontsize=16)\n",
    "\n",
    "fig_1D.tight_layout()\n",
    "\n",
    "if save_plots :\n",
    "    fig_1D.savefig('InputVars_1D.pdf', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Wait with drawing the 2D distribution, so that you think about the 1D distributions first!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two 1D figures, it seems that species A and B can be separated to some degree, but not very well. If you were to somehow select cases of species A, then I can imagine a selection as follows:\n",
    " - If (x0 > 16) or (x1 < 46) or (x0 > 13 and x1 < 52), then guess / select as A.\n",
    "\n",
    "Think about this yourself, and discuss with your peers, how you would go about separating A from B based on x0 and x1.\n",
    "\n",
    " -----------------------  5-10 minutes later  -----------------------\n",
    " \n",
    "As it is, this type of selection is hard to optimise, especially with more dimensions (i.e. more variables than just x0 and x1). That is why Fisher's linear discriminant, $F$, is very useful. It makes the most separating linear combination of the input variables, and the coefficients can be calculated analytically. Thus, it is fast, efficient, and transparent. And it takes linear correlations into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_corr, ax_corr = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax_corr.scatter(spec_A[:, 0], spec_A[:, 1], color='Red',  s=10, label='Species A')\n",
    "ax_corr.scatter(spec_B[:, 0], spec_B[:, 1], color='Blue', s=10, label='Species B')\n",
    "ax_corr.set(xlabel='Parameter x0', ylabel='Parameter x1', title='Correlation');\n",
    "\n",
    "ax_corr.legend();\n",
    "fig_corr.tight_layout()\n",
    "\n",
    "if save_plots :\n",
    "   fig_corr.savefig('InputVars_2D.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Discriminant calculation:\n",
    "\n",
    "We want to find $\\vec{w}$ defined by:\n",
    "\n",
    "$$\\vec{w} = \\left(\\Sigma_A + \\Sigma_B\\right)^{-1} \\left(\\vec{\\mu}_A - \\vec{\\mu}_B\\right)$$\n",
    "\n",
    "which we use to project our data into the best separating plane (line in this case) given by:\n",
    "\n",
    "$$ \\mathcal{F} = w_0 + \\vec{w} \\cdot \\vec{x} $$\n",
    "\n",
    "We start by finding the means and covariance of the individuel species: (__fill in yourself!__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDA(spec_A, spec_B, labels=None):\n",
    "\n",
    "    mu_A = np.mean(spec_A, 0)\n",
    "    mu_B = np.mean(spec_B, 0)\n",
    "\n",
    "    cov_A = np.cov(spec_A.T)\n",
    "    cov_B = np.cov(spec_B.T)\n",
    "    cov_sum = cov_A + cov_B\n",
    "\n",
    "    # inverts cov_sum\n",
    "    cov_sum_inv = np.linalg.inv(cov_sum)\n",
    "\n",
    "    wf = np.dot(cov_sum_inv.T, (mu_A - mu_B))\n",
    "\n",
    "    fisher_data_A = np.dot(wf, spec_A.T)\n",
    "    fisher_data_B = np.dot(wf, spec_B.T)\n",
    "\n",
    "    return fisher_data_A, fisher_data_B, wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_A = np.mean(spec_A, 0)\n",
    "mu_B = np.mean(spec_B, 0) \n",
    "mu_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_A = np.cov(spec_A.T) \n",
    "cov_B = np.cov(spec_B.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_sum = cov_A + cov_B\n",
    "cov_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `cov_sum` is the sum of the all of the species' covariance matrices. We invert this using scipy's `inv` function.  __Note__: fill in yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Delete the definition below of cov_sum when you have filled in the cells above:\n",
    "#cov_sum = np.diag([1, 2])\n",
    "\n",
    "# Inverts cov_sum\n",
    "cov_sum_inv = inv(cov_sum)\n",
    "cov_sum_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the fisher weights, $\\vec{w}$. __Note__: fill in yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = np.dot(cov_sum_inv.T, (mu_A - mu_B)) # fill in yourself\n",
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the fisher discriminant, $\\mathcal{F}$. __Note__: fill in yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_data_A = np.dot(wf, spec_A.T) # fill in yourself\n",
    "fisher_data_B = np.dot(wf, spec_B.T) # fill in yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fisher, ax_fisher = plt.subplots(figsize=(12, 8))\n",
    "fisher_hist_A = ax_fisher.hist(fisher_data_A, 200, range=(-22, 3), histtype='step', color='Red', label='Species A')\n",
    "fisher_hist_B = ax_fisher.hist(fisher_data_B, 200, range=(-22, 3), histtype='step', color='Blue', label='Species B')\n",
    "ax_fisher.set(xlim=(-22, 3), xlabel='Fisher-discriminant')\n",
    "ax_fisher.legend()\n",
    "\n",
    "ax_fisher.text(-21, 60, fr'$\\Delta_{{fisher}} = {calc_separation(fisher_data_A, fisher_data_B):.3f}$', fontsize=16)\n",
    "\n",
    "fig_fisher.tight_layout()\n",
    "\n",
    "if save_plots:\n",
    "    fig_fisher.savefig('FisherOutput.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to visually see the increased seperation (when done correctly). We can also compare $\\Delta_{fisher}$ to $\\Delta_{x0}$ or $\\Delta_{x1}$ and see it clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Questions\n",
    "\n",
    "As always, make sure that you know what the code is doing so far, and what the aim of the exercise is (i.e. which problem to solve, and how). Then start to expand on it. \n",
    "\n",
    "1. Look at the 1D distributions of the two discriminating variables for the two species, and see how well you can separate them by eye. It seems somewhat possible, but certainly far from perfect... Once you consider the 2D distribution (scatter plot - to be uncommented by you!), then it is clear, that some cut along a line at an angle will work much better. This exercise is about finding that optimal line, and thus the perpendicular axis to project the data onto!\n",
    "\n",
    "\n",
    "*Example solution 1:* Well, all the above is visibly true, and not really a question.\n",
    "\n",
    "2. Calculate from the data the mean, widths (std) and covariance of each discriminating variable (pair of variables for covariance) for each species, and put these into the matrices defined.\n",
    "\n",
    "*Example solution 2:* See cells for values.\n",
    "\n",
    "3. From the inverted summed covariance matrices and vectors of means, calculate the two Fisher coefficients, and given these, calculate the Fisher discriminant for the two species in question, i.e. $ \\mathcal{F} = \\vec{w} \\cdot \\vec{x} = w_x \\cdot x + w_y \\cdot y $ for each point (x,y).\n",
    "\n",
    "*Example solution 3:* The fisher coefficients (weights) are calculated in cell 49:  ð‘¤ð´=1.268  and  ð‘¤ðµ=âˆ’0.524 . Given these weights, the fisher value can be calculated. The plot shows that they are very well separated.\n",
    "\n",
    "4. What separation did you get, and is it notably better than what you obtain by eye? Also, do your weights make sense? I.e. are they comparable to the widths of the\n",
    "   corresponding variable? As a simple measure of how good the separation obtained is, we consider the \"distance\" $z$ between the two distributions as a measure of goodness:  \n",
    "   \n",
    "   $$z = \\frac{|\\mu_A-\\mu_B|}{\\sqrt{\\sigma_A^2+\\sigma_B^2}}$$\n",
    "   \n",
    "Compare the separation you get from each of the two 1D histograms of $x_0$ and $x_1$ with what you get from the Fisher discriminant, using the above formula. Of course the ultimate comparison should be done using ROC curves!\n",
    "\n",
    "*Example solution 4:* The final separation is  Î”=2.56 , which corresponds to  2.56ðœŽ  Gaussian separation. Below the corresponding ROC curve can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ROC(hist1, hist2):\n",
    "\n",
    "    # hist1 is signal, hist2 is background\n",
    "\n",
    "    # first we extract the entries (y values) and the edges of the histograms\n",
    "    y_sig, x_sig_edges, __ = hist1\n",
    "    y_bkg, x_bkg_edges, __ = hist2\n",
    "\n",
    "    # Check that the two histograms have the same x edges:\n",
    "    if np.array_equal(x_sig_edges, x_bkg_edges):\n",
    "\n",
    "        # extract the center positions (x values) of the bins (doesn't matter if we use signal or background because they are equal)\n",
    "        x_centers = 0.5*(x_sig_edges[1:] + x_sig_edges[:-1])\n",
    "\n",
    "        # calculate the integral (sum) of the signal and background\n",
    "        integral_sig = y_sig.sum()\n",
    "        integral_bkg = y_bkg.sum()\n",
    "\n",
    "        # initialize empty arrays for the True Positive Rate (TPR) and the False Positive Rate (FPR).\n",
    "        TPR = np.zeros_like(y_sig) # True positive rate (sensitivity)\n",
    "        FPR = np.zeros_like(y_sig) # False positive rate ()\n",
    "\n",
    "        # loop over all bins (x_centers) of the histograms and calculate TN, FP, FN, TP, FPR, and TPR for each bin\n",
    "        for i, x in enumerate(x_centers):\n",
    "\n",
    "            # the cut mask\n",
    "            cut = (x_centers < x)\n",
    "\n",
    "            # true positive\n",
    "            TP = np.sum(y_sig[~cut]) / integral_sig    # True positives\n",
    "            FN = np.sum(y_sig[cut]) / integral_sig     # False negatives\n",
    "            TPR[i] = TP / float(TP + FN)                    # True positive rate\n",
    "\n",
    "            # true negative\n",
    "            TN = np.sum(y_bkg[cut]) / integral_bkg      # True negatives (background)\n",
    "            FP = np.sum(y_bkg[~cut]) / integral_bkg     # False positives\n",
    "            FPR[i] = FP / float(FP + TN)                     # False positive rate\n",
    "\n",
    "        return FPR, TPR\n",
    "\n",
    "    else:\n",
    "        AssertionError(\"Signal and Background histograms have different bins and ranges\")\n",
    "\n",
    "def plot_ROC(FPR, TPR, labels=None, colors=None, figsize=(12,6), ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "\n",
    "    for i in np.arange(len(FPR)):\n",
    "\n",
    "        kwargs = {}\n",
    "        if colors is not None:\n",
    "            kwargs['color'] = colors[i]\n",
    "        if labels is not None:\n",
    "            kwargs['label'] = labels[i]\n",
    "\n",
    "        ax.plot(FPR[i], TPR[i], **kwargs)\n",
    "\n",
    "\n",
    "    ax.plot([0,1], [0,1], 'k--')\n",
    "    if labels is not None:\n",
    "        ax.legend()\n",
    "    ax.set(xlabel='False Positive Rate (Background Efficiency)', ylabel='True Positive Rate (Signal Efficiency)', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr = calc_ROC(fisher_hist_A, fisher_hist_B)\n",
    "plot_ROC([fpr,], [tpr,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Example solution 6_: Let's draw the slope from the weights of the fisher determinent on the scatter plot. For display, we will make the line go through the center of both distribution's mean. By showing the direction of projection (dashed black line perpendicular to the direction of the Fisher discriminant weights), we see that the projection will make a good separation of the two species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_corr, ax_corr = plt.subplots(figsize=(6, 6))\n",
    "ax_corr.scatter(spec_A[:, 0], spec_A[:, 1], color='Red',  s=3, label='Species A')\n",
    "ax_corr.scatter(spec_B[:, 0], spec_B[:, 1], color='Blue', s=3, label='Species B')\n",
    "ax_corr.set(xlabel='Parameter x0', ylabel='Parameter x1', title='Correlation');\n",
    "\n",
    "ax_corr.scatter(mu_A[0], mu_A[1], color=\"cyan\", marker=\"*\", s=100, label=r\"$\\mu_A$\")\n",
    "ax_corr.scatter(mu_B[0], mu_B[1], color=\"pink\", marker=\"*\", s=100, label=r\"$\\mu_B$\")\n",
    "\n",
    "\n",
    "mean_center = 0.5*(mu_A + mu_B)\n",
    "display_length = 10\n",
    "\n",
    "# Define two reference points to display the Discriminant direction axis\n",
    "# wf contains [w_x, w_y]  \n",
    "p1 = mean_center - display_length * wf\n",
    "p2 = mean_center + display_length * wf\n",
    "ax_corr.plot([p1[0], p2[0]], [p1[1], p2[1]], \"k-\", label=\"Fisher discriminant direction\")\n",
    "\n",
    "# Projection: 90 degrees from discriminant direction\n",
    "w_proj = np.array([-wf[1], wf[0]])\n",
    "p3 = mean_center - display_length * w_proj\n",
    "p4 = mean_center + display_length * w_proj\n",
    "ax_corr.plot([p3[0], p4[0]], [p3[1], p4[1]], \"k--\", label=\"Direction of projection\")\n",
    "\n",
    "# Fixing the axes scale to better see the perpendicularity of the projection\n",
    "ax_corr.set_xlim(left=0, right=45)\n",
    "ax_corr.set_ylim(bottom = 30, top=75)\n",
    "\n",
    "ax_corr.legend();\n",
    "fig_corr.tight_layout()\n",
    "\n",
    "if save_plots :\n",
    "   fig_corr.savefig('InputVars_2D_direction.pdf', dpi=600)"
   ]
  }
 ],
 "metadata": {
  "executable": "/usr/bin/env python",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
