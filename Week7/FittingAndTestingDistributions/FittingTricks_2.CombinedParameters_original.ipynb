{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting is an Art!\n",
    "\n",
    "Python macro for testing which fitting procedure is likely to give the \"best\" results.\n",
    "\n",
    "In this case, we consider **Gaussian distributions on constant background (peak fitting and searching)**, where the fit precision and hypothesis testing can be improved by sharing common parameters.\n",
    "\n",
    "### Authors:\n",
    "- Troels Petersen ([email](mailto:petersen@nbi.dk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from iminuit import Minuit, cost\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random\n",
    "r.seed(42)\n",
    "SavePlots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE: Gaussian distributions on a constant background:\n",
    "\n",
    "The initial fitting function is the following:\n",
    "\n",
    "* $f_{1}(x) = \\text{Gauss + Constant} =  \\frac{N_{sig}}{\\sigma\\sqrt(2\\pi)}\\cdot \\exp \\left[-0.5 \\cdot\\left(\\frac{(x-\\mu)}{\\sigma}\\right)^{2} \\right] + C~~~$ for $x$ in $[-\\infty,\\infty]$\n",
    "\n",
    "It disregards that there are additional signal peaks at higher values. Your job is to expand the fit until it really describes the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal parameters:\n",
    "Npoints_gauss = 2000\n",
    "fN     = 0.5\n",
    "mux    = 1.43\n",
    "dmux   = 1.41\n",
    "sigmax = 0.15          \n",
    "\n",
    "# Background parameters:\n",
    "Npoints_pol0 = 5000\n",
    "\n",
    "# Binning parameters:\n",
    "xmin   =  0.0\n",
    "xmax   = 10.0\n",
    "Nbins  =  200\n",
    "binwidth_gauss = (xmax-xmin) / float(Nbins)\n",
    "print(f\"  The bin width is: {binwidth_gauss:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill histogram with signal and background events:\n",
    "Npeak = np.random.geometric(fN, Npoints_gauss)                  # Make random assignment to Gaussian peaks\n",
    "sig = np.random.normal(loc=mux+dmux*(Npeak-1), scale=sigmax)    # Now generate signal according to these peaks\n",
    "bkg = np.random.uniform(xmin, xmax, size=Npoints_pol0)\n",
    "\n",
    "data = np.concatenate([sig, bkg])\n",
    "counts, bin_edges = np.histogram(data, bins=Nbins, range=(xmin, xmax))\n",
    "unc_count = np.sqrt(counts)\n",
    "x = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.errorbar(x, counts, yerr=unc_count, marker = 'o', ls='')\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('Frequency / 0.05', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function (including bin width to get normalisation right):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_gpol0(x, N, mu, sigma, cst) :\n",
    "    norm = binwidth_gauss * N / np.sqrt(2.0*np.pi) / sigma\n",
    "    z = (x-mu)/sigma\n",
    "    return norm * np.exp(-0.5*z*z) + cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = counts>0\n",
    "cfit = cost.LeastSquares(x[mask], counts[mask], unc_count[mask], func_gpol0)\n",
    "mfit = Minuit(cfit, N=1.0, mu=0.0, sigma=1.0, cst=1.0)\n",
    "mfit.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not mfit.fmin.is_valid) :                                   # Check if the fit converged\n",
    "    print(\"  WARNING: The ChiSquare fit DID NOT converge!!!\")\n",
    "\n",
    "ax.plot(x, func_gpol0(x, *mfit.values[:]), 'r', linewidth=2.0, label='Const + Gauss fit')\n",
    "\n",
    "# Adding fit results to plot:\n",
    "chi2 = mfit.fval                     # ChiSquare value\n",
    "Ndof = len(x[mask]) - mfit.nfit      # Number of (non-empty) bins\n",
    "Prob = stats.chi2.sf(chi2, Ndof)     # ChiSquare probability given Ndof\n",
    "\n",
    "fit_info = [f\"$\\\\chi^2$ / $N_\\\\mathrm{{dof}}$ = {chi2:.1f} / {Ndof}\", f\"Prob($\\\\chi^2$, $N_\\\\mathrm{{dof}}$) = {Prob:.3f}\",]\n",
    "for p, v, e in zip(mfit.parameters, mfit.values[:], mfit.errors[:]) :\n",
    "    Ndecimals = max(0,-np.int32(np.log10(e)-1-np.log10(2)))                                # Number of significant digits\n",
    "    fit_info.append(f\"{p} = ${v:{10}.{Ndecimals}{\"f\"}} \\\\pm {e:{10}.{Ndecimals}{\"f\"}}$\")\n",
    "\n",
    "ax.legend(title=\"\\n\".join(fit_info), fontsize=18, title_fontsize = 18, alignment = 'center');\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "\n",
    " 1. Look at the data plot and the corresponding fit. What type of fit is it? Does it\n",
    "    run well (or at all)? And once it runs, does it seem to be reasonable? Why/why not?\n",
    "    What is the p-value from the minimal Chi2 that your fit obtained, once you got any\n",
    "    \"reasonable\" fit to work? Is it acceptable?\n",
    "    Does the fitting function include all features of the data? Why/why not? Try for\n",
    "    2-5 minutes and discuss it with others (or just think for yourself), before reading on!\n",
    "\n",
    "---\n",
    "_2-5 minutes later_...\n",
    "\n",
    "---\n",
    "\n",
    " 2. As it happens, there seem to be a additional peaks. Try to write a new and\n",
    "    expandeded fitting function, which includes these features in the model, and get the\n",
    "    fit to run. How significant is the additional peak(s), based on significance of the\n",
    "    amplitude? And what test would you apply to this, if you wanted to make a full-fledged\n",
    "    hypothesis test between the two models? Are they nested? Can you actually get a number out?\n",
    "    Again, discuss it before coding on.\n",
    "\n",
    "---\n",
    "_10-20 minutes later_...\n",
    "\n",
    "---\n",
    "\n",
    " 3. Imagine that you concluded that there were additional new peaks, and that you were sure that\n",
    "    they had the same width as the original peak (for example because the width was due to\n",
    "    the resolution of the apperatus, and not the peak itself). Does that help you in the fit,\n",
    "    and if so, how? Does the significance of the peaks increase? Would it always do that?\n",
    "    Also imagine, that the parameter of interest is the distance between the peaks. How\n",
    "    would you now write your fitting function?\n",
    "\n",
    " 4. How would you test, if the peaks are really equidistant? Do so...\n",
    "\n",
    " 5. Assuming the peaks to be equidistant, how would you test, if the peaks are consistent with an unobserved peak at exactly zero? Do so...\n",
    "\n",
    " 6. If one wanted to test the G+pol0 vs. the N*G+pol0 models against each other, which might be relevant, then considering the difference in ChiSquare values or -2ln of the likelihood ratio would be obvious (these two gives the same result, when errors are Gaussian and the binning does not have any effect). Wilk's theorem would provide the way to produce a p-value, thus doing a proper hypothesis test using the likelihood ratio test:\n",
    "\n",
    "* Using iminuit, fit the data with both hypothesis, and note the Chi2 or LLH value (using `mfit.fval`).\n",
    "* Then compute the test statistic $\\chi^2_{1} - \\chi^2_{2}$ or $-2\\log{\\frac{LH_{1}}{LH_{2}}}$, and see that it is $\\chi^{2}$ distributed (Wilk's Theorem) from repeating many experiments.\n",
    "\n",
    "NOTE: The test statistic distribution will depend on how many peaks you fit and if you (smartly) eliminated one parameter from the second fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "This exercise in \"fitting tricks\" should teach you that:\n",
    "1. __Good starting values is paramount!__ (Almost all fits fail with poor starting values).\n",
    "2. The form of the fitting function is also important.<br>\n",
    "   a. Ensure that the x-values do not represent some small range far from 0.<br>\n",
    "   b. Ensure that you give the fitting function enough freedom to fit the data.<br>\n",
    "   c. Conversely, try to curb the number of parameters, if there are arguments for doing so (calibration peaks).<br>\n",
    "   d. Make sure that you've normalised your fitting PDFs, to avoid correlations between normalisation and parameters.\n",
    "3. If a fit continues to fail, try simply to draw the function and starting values on top of the data. Often, they don't match well (general advice, not just in this exercise)."
   ]
  }
 ],
 "metadata": {
  "executable": "/usr/bin/env python",
  "kernelspec": {
   "display_name": "appstatfull",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
